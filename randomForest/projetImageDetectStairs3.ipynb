{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qrp8eLtus4Vi",
        "outputId": "87f040e6-1e50-4c3b-8e06-99ad3be5c2ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "   creating: data/test/\n",
            "  inflating: data/test/Groupe1_Image1.jpg  \n",
            "  inflating: data/test/Groupe1_Image4.jpg  \n",
            "  inflating: data/test/Groupe1_Image5.jpg  \n",
            "  inflating: data/test/Groupe1_Image9.jpg  \n",
            "  inflating: data/test/Groupe2_Image1.jpeg  \n",
            "  inflating: data/test/Groupe2_Image11.jpg  \n",
            "  inflating: data/test/Groupe2_Image9.jpg  \n",
            "  inflating: data/test/Groupe5_image04.jpg  \n",
            "  inflating: data/test/Groupe5_image08.jpg  \n",
            "  inflating: data/test/Groupe5_image11.jpeg  \n",
            "  inflating: data/test/Groupe5_image13.jpeg  \n",
            "  inflating: data/test/Groupe5_image17.jpeg  \n",
            "  inflating: data/test/Groupe6_image6.jpg  \n",
            "  inflating: data/test/img4.jpg      \n",
            "  inflating: data/test/img6.jpg      \n",
            "  inflating: data/test/t3i13.jpg     \n",
            "  inflating: data/test/t3i14.jpeg    \n",
            "  inflating: data/test/t3i20.png     \n",
            "  inflating: data/test/t3i21.jpeg    \n",
            "  inflating: data/test/t3i28.jpg     \n",
            "   creating: data/train/\n",
            "  inflating: data/train/Groupe1_Image10.jpg  \n",
            "  inflating: data/train/Groupe1_Image2.jpg  \n",
            "  inflating: data/train/Groupe1_Image3.jpg  \n",
            "  inflating: data/train/Groupe1_Image6.jpg  \n",
            "  inflating: data/train/Groupe1_Image7.jpg  \n",
            "  inflating: data/train/Groupe1_Image8.jpg  \n",
            "  inflating: data/train/Groupe2_Image12.png  \n",
            "  inflating: data/train/Groupe2_Image13.png  \n",
            "  inflating: data/train/Groupe2_Image4.png  \n",
            "  inflating: data/train/Groupe2_Image6.jpeg  \n",
            "  inflating: data/train/Groupe2_Image7.jpeg  \n",
            "  inflating: data/train/Groupe2_Image8.jpg  \n",
            "  inflating: data/train/Groupe5_image01.jpg  \n",
            "  inflating: data/train/Groupe5_image02.jpg  \n",
            "  inflating: data/train/Groupe5_image03.jpg  \n",
            "  inflating: data/train/Groupe5_image05.jpg  \n",
            "  inflating: data/train/Groupe5_image07.jpg  \n",
            "  inflating: data/train/Groupe5_image10.jpeg  \n",
            "  inflating: data/train/Groupe5_image12.jpeg  \n",
            "  inflating: data/train/Groupe5_image14.jpeg  \n",
            "  inflating: data/train/Groupe5_image15.jpeg  \n",
            "  inflating: data/train/Groupe5_image16.jpeg  \n",
            "  inflating: data/train/Groupe6_image1.jpg  \n",
            "  inflating: data/train/Groupe6_image2.jpg  \n",
            "  inflating: data/train/Groupe6_image5.jpg  \n",
            "  inflating: data/train/Groupe6_image7.jpg  \n",
            "  inflating: data/train/Groupe6_image9.jpg  \n",
            "  inflating: data/train/grp7img1.jpeg  \n",
            "  inflating: data/train/grp7img10.jpg  \n",
            "  inflating: data/train/grp7img11.jpg  \n",
            "  inflating: data/train/grp7img12.jpg  \n",
            "  inflating: data/train/grp7img2.jpeg  \n",
            "  inflating: data/train/grp7img3.jpeg  \n",
            "  inflating: data/train/grp7img4.jpeg  \n",
            "  inflating: data/train/grp7img5.jpeg  \n",
            "  inflating: data/train/img1.png     \n",
            "  inflating: data/train/img2.jpg     \n",
            "  inflating: data/train/img3.png     \n",
            "  inflating: data/train/img5.jpg     \n",
            "  inflating: data/train/img8.png     \n",
            "  inflating: data/train/img9.png     \n",
            "  inflating: data/train/t3i12.jpg    \n",
            "  inflating: data/train/t3i15.jpg    \n",
            "  inflating: data/train/t3i17.png    \n",
            "  inflating: data/train/t3i18.png    \n",
            "  inflating: data/train/t3i19.png    \n",
            "  inflating: data/train/t3i2.jpg     \n",
            "  inflating: data/train/t3i22.png    \n",
            "  inflating: data/train/t3i24.jpg    \n",
            "  inflating: data/train/t3i25.jpg    \n",
            "  inflating: data/train/t3i26.jpg    \n",
            "  inflating: data/train/t3i27.jpg    \n",
            "  inflating: data/train/t3i29.jpg    \n",
            "  inflating: data/train/t3i30.jpg    \n",
            "  inflating: data/train/t3i4.jpg     \n",
            "  inflating: data/train/t3i7.jpg     \n",
            "  inflating: data/train/t3i8.jpg     \n",
            "  inflating: data/train/t3i9.jpg     \n",
            "   creating: data/val/\n",
            "  inflating: data/val/Groupe2_Image10.jpg  \n",
            "  inflating: data/val/Groupe2_Image2.jpeg  \n",
            "  inflating: data/val/Groupe2_Image3.jpeg  \n",
            "  inflating: data/val/Groupe2_Image5.jpeg  \n",
            "  inflating: data/val/Groupe5_image06.jpg  \n",
            "  inflating: data/val/Groupe5_image09.jpg  \n",
            "  inflating: data/val/Groupe5_image18.jpeg  \n",
            "  inflating: data/val/Groupe6_image10.jpg  \n",
            "  inflating: data/val/Groupe6_image3.jpg  \n",
            "  inflating: data/val/Groupe6_image4.jpg  \n",
            "  inflating: data/val/Groupe6_image8.jpg  \n",
            "  inflating: data/val/img7.png       \n",
            "  inflating: data/val/t3i1.jpg       \n",
            "  inflating: data/val/t3i10.jpg      \n",
            "  inflating: data/val/t3i11.jpeg     \n",
            "  inflating: data/val/t3i16.png      \n",
            "  inflating: data/val/t3i23.jpg      \n",
            "  inflating: data/val/t3i3.jpg       \n",
            "  inflating: data/val/t3i5.jpg       \n",
            "  inflating: data/val/t3i6.jpg       \n"
          ]
        }
      ],
      "source": [
        "!unzip data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGRNE2yWuShZ",
        "outputId": "102579e1-5e12-4acd-8ab8-e8974a1b565b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Fichier features.csv généré !\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import imutils\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import os\n",
        "import json\n",
        "\n",
        "def find_optimal_canny_threshold(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image\n",
        "    otsu_threshold, _ = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    return int(otsu_threshold * 0.5), int(otsu_threshold * 1.5)\n",
        "\n",
        "def preprocess_image(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image\n",
        "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "    sobel_x = cv2.Sobel(blurred, cv2.CV_16S, 1, 0, ksize=5)\n",
        "    sobel_y = cv2.Sobel(blurred, cv2.CV_16S, 0, 1, ksize=5)\n",
        "    sobel_combined = cv2.addWeighted(cv2.convertScaleAbs(sobel_x), 0.5, cv2.convertScaleAbs(sobel_y), 0.5, 0)\n",
        "    _, binary = cv2.threshold(sobel_combined, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
        "    return binary\n",
        "\n",
        "def detect_dominant_angle(binary):\n",
        "    lines = cv2.HoughLinesP(binary, 1, np.pi / 180, 50, minLineLength=150, maxLineGap=10)\n",
        "    if lines is None:\n",
        "        return 0\n",
        "    angles = [np.arctan2(y2 - y1, x2 - x1) * 180 / np.pi for line in lines for x1, y1, x2, y2 in [line[0]]]\n",
        "    return np.median(angles) if angles else 0\n",
        "\n",
        "def rotate_image(image, angle):\n",
        "    (h, w) = image.shape[:2]\n",
        "    matrix = cv2.getRotationMatrix2D((w // 2, h // 2), -angle, 1.0)\n",
        "    return cv2.warpAffine(image, matrix, (w, h))\n",
        "\n",
        "def detect_stairs(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image\n",
        "    low_threshold, high_threshold = find_optimal_canny_threshold(gray)\n",
        "    edges = cv2.Canny(gray, low_threshold, high_threshold, apertureSize=5)\n",
        "    lines = cv2.HoughLinesP(edges, 1, np.pi / 180, 50, minLineLength=100, maxLineGap=10)\n",
        "    if lines is None:\n",
        "        return 0, 0, 0\n",
        "    detected_lines_y = sorted([(y1 + y2) // 2 for line in lines for x1, y1, x2, y2 in [line[0]]])\n",
        "    merged_lines_y = []\n",
        "    for y in detected_lines_y:\n",
        "        if not merged_lines_y or abs(y - merged_lines_y[-1]) > 50:\n",
        "            merged_lines_y.append(y)\n",
        "    line_distances = np.diff(merged_lines_y) if len(merged_lines_y) > 1 else [0]\n",
        "    return len(merged_lines_y), np.mean(line_distances) if line_distances.any() else 0, np.max(line_distances) if line_distances.any() else 0\n",
        "\n",
        "def process_images(data_path, gt_path):\n",
        "    with open(gt_path, 'r') as f:\n",
        "        ground_truth = json.load(f)\n",
        "\n",
        "    results = []\n",
        "    for entry in ground_truth[\"data\"]:\n",
        "        image_path = os.path.join(data_path, entry[\"image\"])\n",
        "        if not os.path.exists(image_path):\n",
        "            continue\n",
        "\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            continue\n",
        "\n",
        "        image = imutils.resize(image, width=500)\n",
        "        binary = preprocess_image(image)\n",
        "        angle = detect_dominant_angle(binary)\n",
        "        rotated_image = rotate_image(image, -angle) if abs(angle) > 5 else image\n",
        "        num_stairs, avg_dist, max_dist = detect_stairs(rotated_image)\n",
        "\n",
        "        results.append({\n",
        "            \"image\": entry[\"image\"],\n",
        "            \"actual_count\": entry[\"actual_count\"],\n",
        "            \"num_stairs\": num_stairs,\n",
        "            \"avg_step_distance\": avg_dist,\n",
        "            \"max_step_distance\": max_dist\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(\"features.csv\", index=False)\n",
        "    print(\" Fichier features.csv généré !\")\n",
        "\n",
        "process_images(\"data/train\", \"gt.json\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsEwrVP9uSj8",
        "outputId": "129cc168-9130-4006-d3c8-1281de248666"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 Erreur Absolue Moyenne (MAE): 4.08\n",
            "✅ Modèle sauvegardé sous 'stair_detector_model.pkl'\n"
          ]
        }
      ],
      "source": [
        "# Entraînement du modèle Random Forest\n",
        "features_df = pd.read_csv(\"features.csv\")\n",
        "features_df = features_df.drop(columns=[\"image\"])  # Supprimer noms d'image\n",
        "X = features_df.drop(columns=[\"actual_count\"])\n",
        "y = features_df[\"actual_count\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = RandomForestRegressor(n_estimators=500, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\" Erreur Absolue Moyenne (MAE): {mae:.2f}\")\n",
        "joblib.dump(model, \"stair_detector_model.pkl\")\n",
        "print(\" Modèle sauvegardé sous 'stair_detector_model.pkl'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWQGtjaMuSmc",
        "outputId": "5905db82-2fb7-4031-9218-ff1948da872a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 Erreur Absolue Moyenne (MAE) : 5.16\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import imutils\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "#  Charger le modèle entraîné\n",
        "model = joblib.load(\"stair_detector_model.pkl\")\n",
        "\n",
        "def test_model(new_data_path, gt_path=None):\n",
        "    results = []\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    ground_truth = None\n",
        "    if gt_path and os.path.exists(gt_path):\n",
        "        with open(gt_path, 'r') as f:\n",
        "            ground_truth = json.load(f)\n",
        "\n",
        "    for image_name in os.listdir(new_data_path):\n",
        "        image_path = os.path.join(new_data_path, image_name)\n",
        "        if not os.path.isfile(image_path):\n",
        "            continue\n",
        "\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            continue\n",
        "\n",
        "        image = imutils.resize(image, width=500)\n",
        "        binary = preprocess_image(image)\n",
        "        angle = detect_dominant_angle(binary)\n",
        "        rotated_image = rotate_image(image, -angle) if abs(angle) > 5 else image\n",
        "        num_stairs, avg_dist, max_dist = detect_stairs(rotated_image)\n",
        "\n",
        "        feature_data = pd.DataFrame([[num_stairs, avg_dist, max_dist]], columns=X.columns)\n",
        "        predicted_count = model.predict(feature_data)[0]\n",
        "\n",
        "        actual_count, error = None, None\n",
        "        if ground_truth:\n",
        "            for entry in ground_truth[\"data\"]:\n",
        "                if entry[\"image\"] == image_name:\n",
        "                    actual_count = entry[\"actual_count\"]\n",
        "                    error = abs(actual_count - predicted_count) if actual_count is not None else None\n",
        "                    break\n",
        "\n",
        "        results.append({\"image\": image_name, \"actual_count\": actual_count, \"predicted_count\": predicted_count, \"absolute_error\": error})\n",
        "        if actual_count is not None:\n",
        "            y_true.append(actual_count)\n",
        "            y_pred.append(predicted_count)\n",
        "\n",
        "    with open(\"gt_results.json\", \"w\") as f:\n",
        "        json.dump({\"data\": results}, f, indent=4)\n",
        "\n",
        "    if y_true and y_pred:\n",
        "        mae = mean_absolute_error(y_true, y_pred)\n",
        "        print(f\"\\n Erreur Absolue Moyenne (MAE) : {mae:.2f}\")\n",
        "\n",
        "test_model(\"data/test\", \"gt.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV_eA5F1uSpP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
