{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJ8uX5Paa1E4",
        "outputId": "9e93a2b4-54e4-4ffb-b759-3569b58f2914"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "   creating: data/test/\n",
            "  inflating: data/test/Groupe1_Image1.jpg  \n",
            "  inflating: data/test/Groupe1_Image4.jpg  \n",
            "  inflating: data/test/Groupe1_Image5.jpg  \n",
            "  inflating: data/test/Groupe1_Image9.jpg  \n",
            "  inflating: data/test/Groupe2_Image1.jpeg  \n",
            "  inflating: data/test/Groupe2_Image11.jpg  \n",
            "  inflating: data/test/Groupe2_Image9.jpg  \n",
            "  inflating: data/test/Groupe5_image04.jpg  \n",
            "  inflating: data/test/Groupe5_image08.jpg  \n",
            "  inflating: data/test/Groupe5_image11.jpeg  \n",
            "  inflating: data/test/Groupe5_image13.jpeg  \n",
            "  inflating: data/test/Groupe5_image17.jpeg  \n",
            "  inflating: data/test/Groupe6_image6.jpg  \n",
            "  inflating: data/test/img4.jpg      \n",
            "  inflating: data/test/img6.jpg      \n",
            "  inflating: data/test/t3i13.jpg     \n",
            "  inflating: data/test/t3i14.jpeg    \n",
            "  inflating: data/test/t3i20.png     \n",
            "  inflating: data/test/t3i21.jpeg    \n",
            "  inflating: data/test/t3i28.jpg     \n",
            "   creating: data/train/\n",
            "  inflating: data/train/Groupe1_Image10.jpg  \n",
            "  inflating: data/train/Groupe1_Image2.jpg  \n",
            "  inflating: data/train/Groupe1_Image3.jpg  \n",
            "  inflating: data/train/Groupe1_Image6.jpg  \n",
            "  inflating: data/train/Groupe1_Image7.jpg  \n",
            "  inflating: data/train/Groupe1_Image8.jpg  \n",
            "  inflating: data/train/Groupe2_Image12.png  \n",
            "  inflating: data/train/Groupe2_Image13.png  \n",
            "  inflating: data/train/Groupe2_Image4.png  \n",
            "  inflating: data/train/Groupe2_Image6.jpeg  \n",
            "  inflating: data/train/Groupe2_Image7.jpeg  \n",
            "  inflating: data/train/Groupe2_Image8.jpg  \n",
            "  inflating: data/train/Groupe5_image01.jpg  \n",
            "  inflating: data/train/Groupe5_image02.jpg  \n",
            "  inflating: data/train/Groupe5_image03.jpg  \n",
            "  inflating: data/train/Groupe5_image05.jpg  \n",
            "  inflating: data/train/Groupe5_image07.jpg  \n",
            "  inflating: data/train/Groupe5_image10.jpeg  \n",
            "  inflating: data/train/Groupe5_image12.jpeg  \n",
            "  inflating: data/train/Groupe5_image14.jpeg  \n",
            "  inflating: data/train/Groupe5_image15.jpeg  \n",
            "  inflating: data/train/Groupe5_image16.jpeg  \n",
            "  inflating: data/train/Groupe6_image1.jpg  \n",
            "  inflating: data/train/Groupe6_image2.jpg  \n",
            "  inflating: data/train/Groupe6_image5.jpg  \n",
            "  inflating: data/train/Groupe6_image7.jpg  \n",
            "  inflating: data/train/Groupe6_image9.jpg  \n",
            "  inflating: data/train/grp7img1.jpeg  \n",
            "  inflating: data/train/grp7img10.jpg  \n",
            "  inflating: data/train/grp7img11.jpg  \n",
            "  inflating: data/train/grp7img12.jpg  \n",
            "  inflating: data/train/grp7img2.jpeg  \n",
            "  inflating: data/train/grp7img3.jpeg  \n",
            "  inflating: data/train/grp7img4.jpeg  \n",
            "  inflating: data/train/grp7img5.jpeg  \n",
            "  inflating: data/train/img1.png     \n",
            "  inflating: data/train/img2.jpg     \n",
            "  inflating: data/train/img3.png     \n",
            "  inflating: data/train/img5.jpg     \n",
            "  inflating: data/train/img8.png     \n",
            "  inflating: data/train/img9.png     \n",
            "  inflating: data/train/t3i12.jpg    \n",
            "  inflating: data/train/t3i15.jpg    \n",
            "  inflating: data/train/t3i17.png    \n",
            "  inflating: data/train/t3i18.png    \n",
            "  inflating: data/train/t3i19.png    \n",
            "  inflating: data/train/t3i2.jpg     \n",
            "  inflating: data/train/t3i22.png    \n",
            "  inflating: data/train/t3i24.jpg    \n",
            "  inflating: data/train/t3i25.jpg    \n",
            "  inflating: data/train/t3i26.jpg    \n",
            "  inflating: data/train/t3i27.jpg    \n",
            "  inflating: data/train/t3i29.jpg    \n",
            "  inflating: data/train/t3i30.jpg    \n",
            "  inflating: data/train/t3i4.jpg     \n",
            "  inflating: data/train/t3i7.jpg     \n",
            "  inflating: data/train/t3i8.jpg     \n",
            "  inflating: data/train/t3i9.jpg     \n",
            "   creating: data/val/\n",
            "  inflating: data/val/Groupe2_Image10.jpg  \n",
            "  inflating: data/val/Groupe2_Image2.jpeg  \n",
            "  inflating: data/val/Groupe2_Image3.jpeg  \n",
            "  inflating: data/val/Groupe2_Image5.jpeg  \n",
            "  inflating: data/val/Groupe5_image06.jpg  \n",
            "  inflating: data/val/Groupe5_image09.jpg  \n",
            "  inflating: data/val/Groupe5_image18.jpeg  \n",
            "  inflating: data/val/Groupe6_image10.jpg  \n",
            "  inflating: data/val/Groupe6_image3.jpg  \n",
            "  inflating: data/val/Groupe6_image4.jpg  \n",
            "  inflating: data/val/Groupe6_image8.jpg  \n",
            "  inflating: data/val/img7.png       \n",
            "  inflating: data/val/t3i1.jpg       \n",
            "  inflating: data/val/t3i10.jpg      \n",
            "  inflating: data/val/t3i11.jpeg     \n",
            "  inflating: data/val/t3i16.png      \n",
            "  inflating: data/val/t3i23.jpg      \n",
            "  inflating: data/val/t3i3.jpg       \n",
            "  inflating: data/val/t3i5.jpg       \n",
            "  inflating: data/val/t3i6.jpg       \n"
          ]
        }
      ],
      "source": [
        "!unzip data.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import imutils\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def find_optimal_canny_threshold(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image\n",
        "    otsu_threshold, _ = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    return int(otsu_threshold * 0.5), int(otsu_threshold * 1.5)\n",
        "\n",
        "def preprocess_image(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image\n",
        "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "    sobel_x = cv2.Sobel(blurred, cv2.CV_16S, 1, 0, ksize=5)\n",
        "    sobel_y = cv2.Sobel(blurred, cv2.CV_16S, 0, 1, ksize=5)\n",
        "    sobel_combined = cv2.addWeighted(cv2.convertScaleAbs(sobel_x), 0.5, cv2.convertScaleAbs(sobel_y), 0.5, 0)\n",
        "    _, binary = cv2.threshold(sobel_combined, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
        "    return binary\n",
        "\n",
        "def detect_dominant_angle(binary):\n",
        "    lines = cv2.HoughLinesP(binary, 1, np.pi / 180, 50, minLineLength=150, maxLineGap=10)\n",
        "    if lines is None:\n",
        "        return 0\n",
        "    angles = [np.arctan2(y2 - y1, x2 - x1) * 180 / np.pi for line in lines for x1, y1, x2, y2 in [line[0]]]\n",
        "    return np.median(angles) if angles else 0\n",
        "\n",
        "def rotate_image(image, angle):\n",
        "    (h, w) = image.shape[:2]\n",
        "    matrix = cv2.getRotationMatrix2D((w // 2, h // 2), -angle, 1.0)\n",
        "    return cv2.warpAffine(image, matrix, (w, h))\n",
        "\n",
        "def detect_stairs(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image\n",
        "    low_threshold, high_threshold = find_optimal_canny_threshold(gray)\n",
        "    edges = cv2.Canny(gray, low_threshold, high_threshold, apertureSize=5)\n",
        "    lines = cv2.HoughLinesP(edges, 1, np.pi / 180, 50, minLineLength=100, maxLineGap=10)\n",
        "    if lines is None:\n",
        "        return 0, 0, 0\n",
        "    detected_lines_y = sorted([(y1 + y2) // 2 for line in lines for x1, y1, x2, y2 in [line[0]]])\n",
        "    merged_lines_y = []\n",
        "    for y in detected_lines_y:\n",
        "        if not merged_lines_y or abs(y - merged_lines_y[-1]) > 60:\n",
        "            merged_lines_y.append(y)\n",
        "    line_distances = np.diff(merged_lines_y) if len(merged_lines_y) > 1 else [0]\n",
        "    return len(merged_lines_y), np.mean(line_distances) if line_distances.any() else 0, np.max(line_distances) if line_distances.any() else 0\n",
        "\n",
        "def process_images(data_path, gt_path):\n",
        "    with open(gt_path, 'r') as f:\n",
        "        ground_truth = json.load(f)\n",
        "\n",
        "    results = []\n",
        "    for entry in ground_truth[\"data\"]:\n",
        "        image_path = os.path.join(data_path, entry[\"image\"])\n",
        "        if not os.path.exists(image_path):\n",
        "            continue\n",
        "\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            continue\n",
        "\n",
        "        image = imutils.resize(image, width=400)\n",
        "        binary = preprocess_image(image)\n",
        "        angle = detect_dominant_angle(binary)\n",
        "        rotated_image = rotate_image(image, -angle) if abs(angle) > 5 else image\n",
        "        num_stairs, avg_dist, max_dist = detect_stairs(rotated_image)\n",
        "\n",
        "        results.append({\n",
        "            \"image\": entry[\"image\"],\n",
        "            \"actual_count\": entry[\"actual_count\"],\n",
        "            \"predicted_count\": num_stairs,\n",
        "            \"absolute_error\": abs(entry[\"actual_count\"] - num_stairs),\n",
        "            \"avg_step_distance\": avg_dist,\n",
        "            \"max_step_distance\": max_dist\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(\"features.csv\", index=False)\n",
        "    print(\"Fichier features.csv g√©n√©r√© !\")\n",
        "\n",
        "# Ex√©cution\n",
        "process_images(\"data/train\", \"gt.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tbqMXKSa8T7",
        "outputId": "14b2ccc2-a6f2-447f-8304-cbef7800ccc4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fichier features.csv g√©n√©r√© !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Charger les caract√©ristiques extraites\n",
        "features_df = pd.read_csv(\"features.csv\")\n",
        "\n",
        "# Supprimer les colonnes non num√©riques\n",
        "features_df = features_df.drop(columns=[\"image\"])\n",
        "\n",
        "# S√©parer les features et la variable cible\n",
        "X = features_df.drop(columns=[\"actual_count\"])\n",
        "y = features_df[\"actual_count\"]\n",
        "\n",
        "# Diviser les donn√©es en train et test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Entra√Æner un RandomForestRegressor\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Pr√©dictions sur le test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# √âvaluation du mod√®le\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Erreur absolue moyenne (MAE): {mae:.2f}\")\n",
        "\n",
        "# Sauvegarde du mod√®le\n",
        "import joblib\n",
        "joblib.dump(model, \"stair_detector_model.pkl\")\n",
        "print(\"Mod√®le sauvegard√© sous 'stair_detector_model.pkl'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqZMnclva8WS",
        "outputId": "5e9e08aa-1a7f-47a3-ec4e-4284567a4243"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Erreur absolue moyenne (MAE): 1.30\n",
            "Mod√®le sauvegard√© sous 'stair_detector_model.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import imutils\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "# Charger le mod√®le entra√Æn√©\n",
        "model = joblib.load(\"stair_detector_model.pkl\")\n",
        "\n",
        "\n",
        "def test_model(new_data_path, gt_path=None):\n",
        "    results = []\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    # Charger le fichier de v√©rit√© terrain s'il existe\n",
        "    ground_truth = None\n",
        "    if gt_path and os.path.exists(gt_path):\n",
        "        with open(gt_path, 'r') as f:\n",
        "            ground_truth = json.load(f)\n",
        "\n",
        "    for image_name in os.listdir(new_data_path):\n",
        "        image_path = os.path.join(new_data_path, image_name)\n",
        "        if not os.path.isfile(image_path):\n",
        "            continue\n",
        "\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            continue\n",
        "\n",
        "        image = imutils.resize(image, width=400)\n",
        "        binary = preprocess_image(image)\n",
        "        angle = detect_dominant_angle(binary)\n",
        "        rotated_image = rotate_image(image, -angle) if abs(angle) > 5 else image\n",
        "        num_stairs, avg_dist, max_dist = detect_stairs(rotated_image)\n",
        "\n",
        "        # Cr√©er un DataFrame avec les bons noms de colonnes\n",
        "        feature_data = pd.DataFrame([[num_stairs, avg_dist, max_dist]],\n",
        "                                    columns=[\"num_stairs\", \"avg_step_distance\", \"max_step_distance\"])\n",
        "\n",
        "        # V√©rifier que toutes les colonnes sont bien pr√©sentes dans le mod√®le\n",
        "        missing_cols = set(X.columns) - set(feature_data.columns)\n",
        "        for col in missing_cols:\n",
        "            feature_data[col] = 0  # Ajouter les colonnes manquantes avec des valeurs par d√©faut\n",
        "\n",
        "        # R√©ordonner les colonnes pour correspondre √† l'entra√Ænement\n",
        "        feature_data = feature_data[X.columns]\n",
        "\n",
        "        # Pr√©dire le nombre d'escaliers avec le mod√®le\n",
        "        predicted_count = model.predict(feature_data)[0]\n",
        "\n",
        "        actual_count = None\n",
        "        error = None\n",
        "        if ground_truth:\n",
        "            for entry in ground_truth[\"data\"]:\n",
        "                if entry[\"image\"] == image_name:\n",
        "                    actual_count = entry[\"actual_count\"]\n",
        "                    error = abs(actual_count - predicted_count) if actual_count is not None else None\n",
        "                    break\n",
        "\n",
        "        results.append({\n",
        "            \"image\": image_name,\n",
        "            \"actual_count\": actual_count,\n",
        "            \"predicted_count\": predicted_count,\n",
        "            \"absolute_error\": error\n",
        "        })\n",
        "\n",
        "        # Stocker pour calculer la MAE\n",
        "        if actual_count is not None:\n",
        "            y_true.append(actual_count)\n",
        "            y_pred.append(predicted_count)\n",
        "\n",
        "        print(f\"Image: {image_name} | Pr√©dit: {predicted_count:.0f} | R√©el: {actual_count} | Erreur: {error}\")\n",
        "\n",
        "    # Sauvegarder les r√©sultats\n",
        "    with open(\"gt_results.json\", \"w\") as f:\n",
        "        json.dump({\"data\": results}, f, indent=4)\n",
        "\n",
        "    print(\"‚úÖ Test termin√© ! R√©sultats enregistr√©s dans gt_results.json\")\n",
        "\n",
        "    # Calculer la MAE\n",
        "    if y_true and y_pred:\n",
        "        mae = mean_absolute_error(y_true, y_pred)\n",
        "        print(f\"\\nüìä Erreur Absolue Moyenne (MAE) sur l'ensemble test : {mae:.2f}\")\n",
        "\n",
        "# Ex√©cution du test\n",
        "test_model(\"data/test\", \"gt.json\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz8ufoQPdZRG",
        "outputId": "1079e374-4e10-4b1e-b522-5a75d5cb7329"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: img6.jpg | Pr√©dit: 6 | R√©el: 11 | Erreur: 5.18\n",
            "Image: t3i21.jpeg | Pr√©dit: 4 | R√©el: 14 | Erreur: 9.6\n",
            "Image: Groupe1_Image4.jpg | Pr√©dit: 5 | R√©el: 12 | Erreur: 7.08\n",
            "Image: Groupe5_image11.jpeg | Pr√©dit: 6 | R√©el: 13 | Erreur: 7.3\n",
            "Image: Groupe5_image17.jpeg | Pr√©dit: 5 | R√©el: 13 | Erreur: 7.69\n",
            "Image: Groupe1_Image1.jpg | Pr√©dit: 6 | R√©el: 11 | Erreur: 4.84\n",
            "Image: img4.jpg | Pr√©dit: 5 | R√©el: 37 | Erreur: 31.79\n",
            "Image: t3i20.png | Pr√©dit: 5 | R√©el: 2 | Erreur: 2.8600000000000003\n",
            "Image: Groupe5_image13.jpeg | Pr√©dit: 6 | R√©el: 3 | Erreur: 2.88\n",
            "Image: t3i14.jpeg | Pr√©dit: 5 | R√©el: 4 | Erreur: 0.7199999999999998\n",
            "Image: Groupe5_image08.jpg | Pr√©dit: 6 | R√©el: 14 | Erreur: 8.219999999999999\n",
            "Image: Groupe6_image6.jpg | Pr√©dit: 5 | R√©el: 9 | Erreur: 3.79\n",
            "Image: Groupe2_Image11.jpg | Pr√©dit: 6 | R√©el: 9 | Erreur: 3.41\n",
            "Image: Groupe1_Image9.jpg | Pr√©dit: 6 | R√©el: 9 | Erreur: 2.7\n",
            "Image: t3i13.jpg | Pr√©dit: 6 | R√©el: 4 | Erreur: 2.0599999999999996\n",
            "Image: Groupe1_Image5.jpg | Pr√©dit: 6 | R√©el: 12 | Erreur: 5.7\n",
            "Image: Groupe2_Image9.jpg | Pr√©dit: 6 | R√©el: 9 | Erreur: 2.83\n",
            "Image: Groupe5_image04.jpg | Pr√©dit: 6 | R√©el: 6 | Erreur: 0.16999999999999993\n",
            "Image: t3i28.jpg | Pr√©dit: 6 | R√©el: 8 | Erreur: 1.7000000000000002\n",
            "Image: Groupe2_Image1.jpeg | Pr√©dit: 6 | R√©el: 4 | Erreur: 1.5899999999999999\n",
            "‚úÖ Test termin√© ! R√©sultats enregistr√©s dans gt_results.json\n",
            "\n",
            "üìä Erreur Absolue Moyenne (MAE) sur l'ensemble test : 5.61\n"
          ]
        }
      ]
    }
  ]
}